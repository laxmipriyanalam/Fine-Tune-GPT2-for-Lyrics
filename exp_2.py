# -*- coding: utf-8 -*-
"""exp 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15SDDgC8ASxN-K6cNbf2Lm1otRaR--4Je
"""

!pip install datasets

import os
os.environ["WANDB_DISABLED"] = "true"

import pandas as pd
from datasets import Dataset
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from transformers import pipeline
import torch
import os

# Load dataset
df = pd.read_excel("./TaylorSwiftSongs.xlsx")
df = df.dropna(subset=["LYRICS"])
df = df[df["LYRICS"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]
lyrics_texts = df["LYRICS"].tolist()

# Create HuggingFace Dataset
dataset = Dataset.from_dict({"text": lyrics_texts})

# Load GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # GPT2 doesn't have pad token by default
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Tokenization
def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

tokenized_dataset = dataset.map(tokenize, batched=True)

# Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-taylorswift",
    overwrite_output_dir=True,
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_steps=50,
    save_steps=200,
    save_total_limit=1,
    prediction_loss_only=True,
    fp16=torch.cuda.is_available()
)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Train model
trainer.train()

# Save model
trainer.save_model("./gpt2-taylorswift")
tokenizer.save_pretrained("./gpt2-taylorswift")

# Sample predictions
generator = pipeline('text-generation', model='./gpt2-taylorswift', tokenizer=tokenizer)


prompts = [
    "We were both young when I first saw you",
    "I remember tears streaming down your face",
    "Loving him is like driving a new Maserati",
    "we were both young when I first saw you",
]

for prompt in prompts:
    print(f"\nPrompt: {prompt}")
    output = generator(prompt, max_length=100, num_return_sequences=1)[0]["generated_text"]
    print(f"Generated Lyrics:\n{output}")

generator = pipeline('text-generation', model='./gpt2-taylorswift', tokenizer=tokenizer)


prompts = [
    "Nice to meet you,where you been",
    "I'm pretty sure we broke up last night",
]

for prompt in prompts:
    print(f"\nPrompt: {prompt}")
    output = generator(prompt, max_length=100, num_return_sequences=1)[0]["generated_text"]
    print(f"Generated Lyrics:\n{output}")

